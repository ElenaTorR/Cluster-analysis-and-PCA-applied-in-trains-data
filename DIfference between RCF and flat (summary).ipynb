{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from pandas import Series\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import svm\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML, Image\n",
    "pd.options.display.max_columns = None\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : DATA SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with flat test cases (only data with speed > 60 and vibrations above 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_flat = 'S:\\\\Analytics\\\\TemporaryThings\\\\difference between RCF and flats\\\\Flat\\\\'\n",
    "dataframe_dic_flat = {} # dataframe dictionary to fill in with data \n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "for i in listdir(folder_flat):\n",
    "    tmp = i.split('Export_')[-1].split('.csv')[0]\n",
    "    fin = join(folder_flat,i)\n",
    "    dataframe_dic_flat[tmp]=pd.read_csv(\n",
    "        fin, \n",
    "        index_col = False,\n",
    "        usecols = ['UTC Time Date','BHI','WHI','Speed','RMS X','RMS Y','RMS Z','Peak X','Peak Y','Peak Z','Speed','RSSI','Ave Temp C','Temp Diff C','Voltage','Latitude','Longitude','Speed','Altitude','Heading','X Offset','Y Offset','Z Offset'],\n",
    "        parse_dates = ['UTC Time Date'],date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.concat(dataframe_dic_flat)\n",
    "df_flat = df_flat.reset_index().rename(columns ={'level_0':'Wheel','Peak X':'PeakX'}).drop(['level_1'],axis = 1)\n",
    "df_flat = df_flat.loc[(df_flat['Speed']>60) & (df_flat['PeakX']> 0) & (df_flat['Peak Y']> 0) & (df_flat['Peak Z']>0) & (df_flat['RMS X']>0) & (df_flat['RMS Y']>0)  & (df_flat['RMS Z']>0)]\n",
    "df_flat['ID'] = np.zeros(len(df_flat))\n",
    "df_flat['EHI'] = (np.power(df_flat['RMS X'],2)+np.power(df_flat['RMS Y'],2)+np.power(df_flat['RMS Z'],2))/np.power(df_flat['Speed'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with RCF test cases (only data with speed > 60 and vibrations above 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_RCF = 'S:\\\\Analytics\\\\TemporaryThings\\\\difference between RCF and flats\\\\RCF\\\\'\n",
    "dataframe_dic_RCF = {} \n",
    "for i in listdir(folder_RCF):\n",
    "    tmp = i.split('Export_')[-1].split('.csv')[0]\n",
    "    fin = join(folder_RCF,i)\n",
    "    dataframe_dic_RCF[tmp]=pd.read_csv(\n",
    "        fin, \n",
    "        index_col = False,\n",
    "        usecols = ['UTC Time Date','BHI','WHI','Speed','RMS X','RMS Y','RMS Z','Peak X','Peak Y','Peak Z','Speed','RSSI','Ave Temp C','Temp Diff C','Voltage','Latitude','Longitude','Speed','Altitude','Heading','X Offset','Y Offset','Z Offset'],\n",
    "        parse_dates = ['UTC Time Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RCF = pd.concat(dataframe_dic_RCF)\n",
    "df_RCF = df_RCF.reset_index().rename(columns ={'level_0':'Wheel','Peak X':'PeakX'}).drop(['level_1'],axis = 1)\n",
    "df_RCF = df_RCF.loc[(df_RCF['Speed']>60) & (df_RCF['PeakX']> 0) & (df_RCF['Peak Y']> 0) & (df_RCF['Peak Z']>0) & (df_RCF['RMS X']>0) & (df_RCF['RMS Y']>0)  & (df_RCF['RMS Z']>0)]\n",
    "df_RCF['ID'] = np.full((len(df_RCF), 1),1)\n",
    "df_RCF['EHI'] = (np.power(df_RCF['RMS X'],2)+np.power(df_RCF['RMS Y'],2)+np.power(df_RCF['RMS Z'],2))/np.power(df_RCF['Speed'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHI and EHI time series FLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vals in df_flat.groupby('Wheel'):\n",
    "    df = df_flat.loc[(df_flat.Wheel ==  i)]\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(df['UTC Time Date'],df['WHI'])\n",
    "    plt.plot(df['UTC Time Date'],df['EHI']*100)\n",
    "    plt.legend([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHI and EHI time series RCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vals in df_RCF.groupby('Wheel'):\n",
    "    df = df_RCF.loc[(df_RCF.Wheel ==  i)]\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(df['UTC Time Date'],df['WHI'])\n",
    "    #plt.figure(figsize = (30,10))\n",
    "    plt.plot(df['UTC Time Date'],df['EHI']*100)\n",
    "    plt.legend([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation between dataframe with flat cases and RCF cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_flat,df_RCF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  a new dataset with the date of maximum WHI for each test cases (wheel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElenaR\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WHI_max = {}\n",
    "WHI_quantile = {}\n",
    "df = df\n",
    "for (v,d),vals in df.groupby(['Wheel','ID']):\n",
    "    WHI_max[v,d] = df.loc[(df.WHI == vals['WHI'].max())]\n",
    "    WHI_max[v,d]['UTC Time Date'] = pd.to_datetime(WHI_max[v,d]['UTC Time Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_WHI = pd.concat(WHI_max).reset_index().drop(['level_0','level_1','level_2'],axis = 1).rename(columns = {'WHI': 'WHI_max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate the dataset with the date of maximum WHI and the dataframe with flat and RCF cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = pd.merge(df,max_WHI[['Wheel','ID','UTC Time Date','WHI_max']],on = ['Wheel','ID']).rename(columns ={'UTC Time Date_x' : 'Date','UTC Time Date_y' : 'Date_max_WHI'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data with WHI high with value closes to the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_range ={}\n",
    "for (w,ind,maxm),vals in df_max.groupby(['Wheel','ID','Date_max_WHI']):\n",
    "    if ind == 1:\n",
    "        df_high_range[w,ind] = df_max.loc[(df_max.Wheel == w) & (df_max.ID == ind) & (df_max.Date <= maxm) & (df_max.WHI >= vals['WHI'].quantile(0.85))]\n",
    "    else:\n",
    "        df_high_range[w,ind] = df_max.loc[(df_max.Wheel == w) & (df_max.ID == ind) & (df_max.Date >= maxm - timedelta(days = 7)) & (df_max.Date <= maxm) & (df_max.WHI >= vals['WHI'].quantile(0.85))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_WHI = pd.concat(df_high_range).reset_index().drop(['level_0','level_1','level_2'],axis = 1)\n",
    "df_high_WHI['Costumer'],df_high_WHI['Vehicle'],df_high_WHI['Unit'],df_high_WHI['Node'] = df_high_WHI['Wheel'].str.split('_').str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_WHI = df_high_WHI[['Wheel','ID','Date','RSSI', 'Ave Temp C', 'Temp Diff C','BHI','WHI','Voltage','Speed','X Offset','Y Offset','Z Offset','RMS X','RMS Y','RMS Z', 'PeakX', 'Peak Y','Peak Z','EHI']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot WHI only for the date where it is high (flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vals in df_high_WHI.loc[(df_high_WHI.ID ==0)].groupby('Wheel'):\n",
    "    df = df_high_WHI.loc[(df_high_WHI.ID ==0)].loc[(df_high_WHI.loc[(df_high_WHI.ID ==0)].Wheel ==  i)]\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(df['Date'],df['WHI'])\n",
    "    plt.legend([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot WHI only for the date where it is high (RCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vals in df_high_WHI.loc[(df_high_WHI.ID ==1)].groupby('Wheel'):\n",
    "    df = df_high_WHI.loc[(df_high_WHI.ID ==1)].loc[(df_high_WHI.loc[(df_high_WHI.ID ==1)].Wheel ==  i)]\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(df['Date'],df['WHI'])\n",
    "    plt.legend([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 : ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the vibrations average,25 and 85 percentile per day (flat cases) or per 14 days (RCF cases) and generate a new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = {}\n",
    "high_percentile = {}\n",
    "low_percentile = {}\n",
    "\n",
    "for k in df_high_WHI.ID.unique():\n",
    "    if k == 0: \n",
    "        average[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='D')]).mean().reset_index()\n",
    "        high_percentile[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='D')]).quantile(0.85).reset_index()\n",
    "        low_percentile[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='D')]).quantile(0.25).reset_index()\n",
    "    if k == 1:\n",
    "        average[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='14D')]).mean().reset_index()\n",
    "        high_percentile[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='14D')]).quantile(0.85).reset_index()\n",
    "        low_percentile[k] = df_high_WHI.loc[(df_high_WHI.ID == k)].groupby(['Wheel',pd.Grouper(key = 'Date',freq='14D')]).quantile(0.25).reset_index()\n",
    "        \n",
    "average = pd.concat(average, sort=False)\n",
    "high_percentile = pd.concat(high_percentile, sort=False)\n",
    "low_percentile = pd.concat(low_percentile, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = pd.merge(high_percentile,low_percentile,on = ['Wheel','ID'])\n",
    "final_df = pd.merge(percentile,average,on = ['Wheel','ID']).drop(['Date_x','Date_y'],axis = 1)\n",
    "final_df.columns = final_df.columns.str.replace('_x','_85 percentile')\n",
    "final_df.columns = final_df.columns.str.replace('_y','_25 percentile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(final_df['Wheel'].unique(),test_size=0.2)\n",
    "df_train_l = {}\n",
    "df_test_l = {}\n",
    "for t in train:\n",
    "    df_train_l[t] = final_df.loc[final_df['Wheel']==t]\n",
    "for te in test:\n",
    "    df_test_l[te] = final_df.loc[final_df['Wheel']==te]\n",
    "\n",
    "df_train = pd.concat(df_train_l).reset_index().drop(['level_0','level_1'],axis=1) \n",
    "df_test = pd.concat(df_test_l).reset_index().drop(['level_0','level_1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5737 1918\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[df_train['ID'] == 0]),len(df_test[df_test['ID'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19282 6764\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[df_train['ID'] == 1]),len(df_test[df_test['ID'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison variable distribution for flat and RCF TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_sel = ['PeakX_85 percentile','Peak Y_85 percentile','Peak Z_85 percentile','RMS X_85 percentile','RMS Y_85 percentile','RMS Z_85 percentile','PeakX_25 percentile','Peak Y_25 percentile','Peak Z_25 percentile','RMS X_25 percentile','RMS Y_25 percentile','RMS Z_25 percentile',\n",
    "                'PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z','EHI','EHI_85 percentile','EHI_25 percentile']\n",
    "\n",
    "f = 20\n",
    "flat = df_train[df_train['ID'] == 0]\n",
    "RCF = df_train[df_train['ID'] == 1]\n",
    "for i in features_sel:\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(40, 10))\n",
    "    axs[0].hist(flat[i],bins=40,color='r',alpha=.5,density = True)\n",
    "    axs[0].hist(RCF[i],bins=40,color='g',alpha=0.3,density = True)\n",
    "    axs[0].legend(['flat','RCF'],fontsize = f)\n",
    "    axs[0].set_title(' '.join([i,'Train','distribution']),fontsize = f)\n",
    "    axs[0].tick_params(axis=\"x\", labelsize=f)\n",
    "    axs[0].tick_params(axis=\"y\", labelsize=f)\n",
    "    axs[1].hist(flat[i],density=True, cumulative=True, label='CDF',histtype='step', alpha=0.8, color='r')\n",
    "    axs[1].hist(RCF[i],density=True, cumulative=True, label='CDF',histtype='step', alpha=0.8, color='g')\n",
    "    axs[1].set_title(' '.join([i,'Train','cumulative distribution function']),fontsize = f)\n",
    "    axs[1].legend(['flat','RCF'],fontsize = f)\n",
    "    axs[1].tick_params(axis=\"x\", labelsize=f)\n",
    "    axs[1].tick_params(axis=\"y\", labelsize=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison variable distribution for flat and RCF TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 20\n",
    "flat = df_test[df_test['ID'] == 0]\n",
    "RCF = df_test[df_test['ID'] == 1]\n",
    "for i in features_sel:\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(40, 10))\n",
    "    axs[0].hist(flat[i],bins=40,color='r',alpha=.5,density = True)\n",
    "    axs[0].hist(RCF[i],bins=40,color='g',alpha=0.3,density = True)\n",
    "    axs[0].legend(['flat','RCF'],fontsize = f)\n",
    "    axs[0].set_title(' '.join([i,'Test','distribution']),fontsize = f)\n",
    "    axs[0].tick_params(axis=\"x\", labelsize=f)\n",
    "    axs[0].tick_params(axis=\"y\", labelsize=f)\n",
    "    axs[1].hist(flat[i],density=True, cumulative=True, label='CDF',histtype='step', alpha=0.8, color='r')\n",
    "    axs[1].hist(RCF[i],density=True, cumulative=True, label='CDF',histtype='step', alpha=0.8, color='g')\n",
    "    axs[1].set_title(' '.join([i,'Test','cumulative distribution function']),fontsize = f)\n",
    "    axs[1].legend(['flat','RCF'],fontsize = f)\n",
    "    axs[1].tick_params(axis=\"x\", labelsize=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFERENCE OF DISTRIBUTION FOR RCF CASES BASED ON RAW VIBRATIONS VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = df_train[df_train['ID'] == 1].Wheel.unique()\n",
    "comp = []\n",
    "n = 6\n",
    "lists = [[] for _ in range(n)]\n",
    "for x in seq: \n",
    "    for y in seq:\n",
    "        if x !=y:\n",
    "            comp.append((x,y))\n",
    "comp = pd.DataFrame(comp)\n",
    "\n",
    "for i in range(0,len(comp)):\n",
    "    for n,p in enumerate(['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "        lists[n].append(stats.ks_2samp(df_train.loc[(df_train['ID'] == 1) & (df_train['Wheel'] == comp[0][i])][p], df_train.loc[(df_train['ID'] == 1) & (df_train['Wheel'] == comp[1][i])][p])[0])\n",
    "    \n",
    "for n,p in enumerate(['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "    comp[p] = lists[n] \n",
    "comp['Damage'] = 'RCF'\n",
    "comp = comp.rename(columns = {0: 'Wheel1',1:'Wheel2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCF CLUSTER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9191612  0.92531932 0.91863944 0.93497101 0.94990007 0.95284   ]\n",
      " [0.49035035 0.48608323 0.48098748 0.52136661 0.48957771 0.52427837]\n",
      " [0.60032381 0.65609355 0.7958312  0.6701965  0.71585198 0.85690238]\n",
      " [0.73845476 0.68573947 0.51255628 0.7658816  0.69446061 0.62155245]]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(comp.iloc[:,2:8])\n",
    "print(kmeans.cluster_centers_) # print location of clusters learned by kmeans object\n",
    "y_km = kmeans.fit_predict(comp.iloc[:,2:8]) # save new clusters for chart\n",
    "comp['cluster'] = list(y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTER ANALYSIS VISUALIZATION-RCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z','cluster']\n",
    "f = ['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']\n",
    "plt.figure(figsize = (20,10))\n",
    "for n,k in enumerate(['red','black','blue','cyan']):\n",
    "    plt.scatter(comp[g].loc[(comp[g].cluster == n)]['PeakX'], comp[g].loc[(comp[g].cluster == n)]['Peak Y'], s=100, c=k)\n",
    "    plt.scatter(comp[g].loc[(comp[g].cluster == n)]['Peak Z'], comp[g].loc[(comp[g].cluster == n)]['RMS X'], s=100, c=k)\n",
    "    plt.scatter(comp[g].loc[(comp[g].cluster == n)]['RMS Y'], comp[g].loc[(comp[g].cluster == n)]['RMS Z'], s=100, c=k)\n",
    "plt.title('RCF cluster output based on vibrations distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFERENCE OF DISTRIBUTION FOR FLATS CASES BASED ON RAW VIBRATIONS VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = df_train[df_train['ID'] == 0].Wheel.unique()\n",
    "comp2 = []\n",
    "n = 6\n",
    "lists = [[] for _ in range(n)]\n",
    "for x in seq: \n",
    "    for y in seq:\n",
    "        if x !=y:\n",
    "            comp2.append((x,y))\n",
    "comp2= pd.DataFrame(comp2)\n",
    "\n",
    "for i in range(0,len(comp2)):\n",
    "    for n,p in enumerate(['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "        lists[n].append(stats.ks_2samp(df_train.loc[(df_train['ID'] == 0) & (df_train['Wheel'] == comp2[0][i])][p], df_train.loc[(df_train['ID'] == 0) & (df_train['Wheel'] == comp2[1][i])][p])[0])\n",
    "    \n",
    "for n,p in enumerate(['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']):\n",
    "    comp2[p] = lists[n]  \n",
    "comp2['Damage'] = 'Flat'\n",
    "comp2 = comp.rename(columns = {0: 'Wheel1',1:'Wheel2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLATS CLUSTER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60178146 0.68156799 0.8067221  0.66536591 0.74044931 0.85666874]\n",
      " [0.48720426 0.48577638 0.48514767 0.50786418 0.48960214 0.52122632]\n",
      " [0.92415094 0.92627142 0.90913801 0.93972538 0.95055629 0.94495339]\n",
      " [0.71248716 0.64408269 0.51256411 0.76369198 0.65463754 0.63628882]]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(comp2.iloc[:,2:8])\n",
    "print(kmeans.cluster_centers_) # print location of clusters learned by kmeans object\n",
    "y_km = kmeans.fit_predict(comp2.iloc[:,2:8]) # save new clusters for chart\n",
    "comp2['cluster'] = list(y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTER ANALYSIS VISUALIZATION-FLATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z','cluster']\n",
    "f = ['PeakX','Peak Y','Peak Z','RMS X','RMS Y','RMS Z']\n",
    "plt.figure(figsize = (20,10))\n",
    "for n,k in enumerate(['red','black','blue','cyan']):\n",
    "    plt.scatter(comp2[g].loc[(comp2[g].cluster == n)]['PeakX'], comp2[g].loc[(comp2[g].cluster == n)]['Peak Y'], s=100, c=k)\n",
    "    plt.scatter(comp2[g].loc[(comp2[g].cluster == n)]['Peak Z'], comp2[g].loc[(comp2[g].cluster == n)]['RMS X'], s=100, c=k)\n",
    "    plt.scatter(comp2[g].loc[(comp2[g].cluster == n)]['RMS Y'], comp2[g].loc[(comp2[g].cluster == n)]['RMS Z'], s=100, c=k)\n",
    "    plt.title('Flat cluster output based on vibrations distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on raw vibrations the results are not good. Make the same analysis using the percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_X = preprocessing.normalize(df_train[features_sel])\n",
    "normalized_test_X = preprocessing.normalize(df_test[features_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42289642 0.31350679 0.13370735]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "principalComponents = pca.fit_transform(normalized_train_X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "finalDf = pd.concat([principalDf, df_train[['ID']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and components correlation matrix TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 30\n",
    "fig, ax = plt.subplots(figsize=(40,10))\n",
    "ax.matshow(pca.components_,cmap='viridis')\n",
    "ax.set_yticklabels(['']+['1st Comp','2nd Comp','3rd Comp'],fontsize=f)\n",
    "fig.colorbar(ax.matshow(pca.components_,cmap='viridis')).ax.tick_params(labelsize=f)\n",
    "plt.title('Correlation matrix between components and features: Train set', y=-0.4,fontsize=f)\n",
    "plt.xticks(range(len(features_sel)),features_sel,rotation=65,ha='left',fontsize=f)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPALINED VARIANCE RATIO TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 30\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.grid()\n",
    "pca = PCA().fit(normalized_train_X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.axvline(x = 3,color = 'r')\n",
    "plt.xticks(arange(24, step=1),fontsize = f)\n",
    "plt.yticks(fontsize = f)\n",
    "plt.title('Explained variance ratio: Train set',fontsize = f)\n",
    "plt.xlabel('number of components',fontsize = f)\n",
    "plt.ylabel('cumulative explained variance',fontsize = f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transform to both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = pca.transform(normalized_train_X)\n",
    "test_t = pca.transform(normalized_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Logistic Regression to the Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_t,df_train['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEASURE COMPONENT PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7719419488597098"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.score(test_t,df_test['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.01      0.02      1918\n",
      "         1.0       0.78      0.99      0.87      6764\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      8682\n",
      "   macro avg       0.49      0.50      0.45      8682\n",
      "weighted avg       0.65      0.77      0.68      8682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_l = logisticRegr.predict(test_t)\n",
    "print(classification_report(df_test['ID'], y_pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(df_test['ID'], y_pred_l)\n",
    "sns.heatmap(mat.T, square=True, annot=True,cmap='coolwarm', fmt='d', cbar=True)\n",
    "plt.title('Logistic regression with components')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST TWO COMPONENTS VISUALIZATION TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 30\n",
    "fig = plt.figure(figsize = (40,20))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = f)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = f)\n",
    "ax.set_title('First 2 PCA Component', fontsize = f)\n",
    "targets = [0,1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['ID'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 70)\n",
    "ax.legend(['Wheel_flat','RCF'],fontsize = 30)\n",
    "plt.xticks(fontsize = f)\n",
    "plt.yticks(fontsize = f)\n",
    "plt.title('PCA results: Training set',fontsize = f)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT ANHD NORMALIZED THE FATURES WITH THE MAIN IMPACT IN THE COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_m = ['PeakX','Peak Y','PeakX_25 percentile','Peak Y_25 percentile','PeakX_85 percentile','Peak Y_85 percentile']\n",
    "normalized_train_X_m = preprocessing.normalize(df_train[features_m])\n",
    "normalized_test_X_m = preprocessing.normalize(df_test[features_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM EXTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='poly',degree = 4,gamma='scale',class_weight = 'balanced')\n",
    "clf.fit(normalized_train_X_m, df_train['ID'])\n",
    "y_pred = clf.predict(normalized_test_X_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: EVALUATING THE ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.86      0.55      1918\n",
      "         1.0       0.94      0.64      0.76      6764\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      8682\n",
      "   macro avg       0.67      0.75      0.66      8682\n",
      "weighted avg       0.82      0.69      0.72      8682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['ID'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(df_test['ID'], y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True,cmap='coolwarm', fmt='d', cbar=True)\n",
    "plt.title('SVM Model')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION WITH ORIGINAL FEATURES WITH THE MAIN IMPACT IN THE COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.80      0.48      1918\n",
      "         1.0       0.91      0.57      0.70      6764\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      8682\n",
      "   macro avg       0.63      0.69      0.59      8682\n",
      "weighted avg       0.79      0.62      0.65      8682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression(solver = 'saga',class_weight = 'balanced')\n",
    "logisticRegr.fit(normalized_train_X_m,df_train['ID'])\n",
    "y_pred_log = logisticRegr.predict(normalized_test_X_m)\n",
    "print(classification_report(df_test['ID'], y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(df_test['ID'], y_pred_log)\n",
    "sns.heatmap(mat.T, square=True, annot=True,cmap='coolwarm', fmt='d', cbar=True)\n",
    "plt.title('Logistic regression')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
